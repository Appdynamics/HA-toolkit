$Id: APPD_UPGRADE 3.26 2017-08-09 14:44:58 cmayer Exp $

This document describes the best practice workflow for upgrading the
controllers in an HA cluster.  In the below workflow, the distinction
is made between secondary and primary.   primary and secondary are 
transient roles that are switched using failover.  Also note that if you
use a different privilege escalation method, replace the invocations of
'sudo service' with /sbin/appdservice or the appropriate substitute.

1) check that Load Balancer correctly detects server liveness via calls 
   to <controller>:<port>/controller/rest/serverstatus that respond with 
   text containing <available>true - otherwise the following upgrade process
   that starts Glassfish on the secondary for a time may wrongly be sent agent
   or UI traffic
  
2) begin maintenance window (declare AppD service down)

3) on primary: stop Glassfish 
   sudo service appdcontroller stop

4) on secondary: stop watchdog and Glassfish:
  sudo service appdcontroller stop

5) on secondary: monitor HA/appdstatus.sh until it reports 
   Seconds_Behind_Master: 0

6) on secondary: stop MySQL
   sudo service appdcontroller-db stop

7) now secondary is unaffected by changes on primary so it is a good backup 
   in case upgrade fails. Secondary is up to the minute copy of primary.

8) on primary: upgrade controller
   this updates the database schema and controller binaries only on the
   primary, since mysql replication is paused by the stopping of the mysql
   instance on the secondary.

9) on primary: stop controller appserver and database
    sudo service appdcontroller-db stop

10) on primary: compare pre & post upgraded domain.xml & db.cnf configs and 
    add in any local changes that were not preserved during upgrade. The
    secondary will contain the pre-upgrade files.

11) on primary: start the controller
    sudo service appdcontroller start

12) validate installation
    Once Glassfish up, login to UI via browser directly to server, 
    if all looks good logout. Verify that curl 
       <controller>:<port>/controller/rest/serverstatus returns 
    updated version and <available>true<available>. 

13) if upgrade is deemed unsuccessful on the primary, proceed to step FAIL 1
    debug of the upgrade may be profitable if small configuration steps were
    incorrectly performed; the installer may be run repeatedly to repair the
    upgrade if needed.

--------- PROCEED HERE ONLY IF UPGRADE GOOD --------------

14) on secondary: start database 
    sudo service appdcontroller-db start

15) on secondary: wait for database to apply schema and other changes
    monitor HA/appdstatus.sh until it reports Seconds_Behind_Master: 0

16) on secondary: stop MySQL
    sudo service appdcontroller-db stop

17) on secondary: upgrade controller
    since the database is marked passive, no changes are applied to the
    database

18) on secondary: stop controller and database
    sudo service appdcontroller-db stop

19) on secondary: apply changes to domain.xml and db.cnf
    these files may be copied from the primary

20) on secondary: start the database and controller
    sudo service appdcontroller-db start
    bin/controller.sh start-appserver

21) on secondary: verify appserver startup
    verify that curl <controller>:<port>/controller/rest/serverstatus 
    returns updated version and <available>false<available>.

22) on secondary; stop secondary appserver
    bin/controller.sh stop-appserver

--------- PROCEED HERE ONLY IF UPGRADE BAD --------------

FAIL 1) on the primary:  stop everything
    sudo appdcontroller-db stop

FAIL 2) on the secondary:  start the database and fail over
    sudo appdcontroller-db start
    HA/failover.sh

FAIL 3) replicate over the failed original primary and re-attempt
    upgrade the after root cause for the failed upgrade is determined.
